{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a02b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- CONFIG -------------------------\n",
    "OUTPUT_DIR = \"sec_swot_output\"  # where to save CSVs + JSONs\n",
    "PORTFOLIO_DIR = \"sec_portfolio\"  # datamule Portfolio working directory\n",
    "TICKERS = [\"AAPL\"]  # modify: list of tickers to download\n",
    "FORMS = [\"10-K\"]\n",
    "DATE_RANGE = (\"2023-01-01\", \"2024-12-31\")  # (start_date, end_date) or None\n",
    "MAX_SENTENCE_LENGTH = 500\n",
    "MIN_SENTENCE_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568316d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- IMPORTS -------------------------\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# local optional imports\n",
    "try:\n",
    "    from datamule import Portfolio\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"datamule is required. Install with: pip install datamule\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# try to import transformers + torch for zero-shot; fallback to weak supervision\n",
    "candidate_labels = [\"Strength\", \"Weakness\", \"Opportunity\", \"Threat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a028983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- UTILITIES -------------------------\n",
    "\n",
    "def ensure_dir(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Normalize whitespace and remove control chars\n",
    "    t = re.sub(r\"[\\r\\x0c]+\", \"\\n\", text)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = t.strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    # naive punctuation-based split; good starting point\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    out = []\n",
    "    for s in sents:\n",
    "        s = s.strip()\n",
    "        if len(s) >= MIN_SENTENCE_LENGTH and len(s) <= MAX_SENTENCE_LENGTH:\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "\n",
    "def extract_text_from_contents(contents):\n",
    "    \"\"\"Recursively extract strings from nested datamule 'contents' dicts or lists.\"\"\"\n",
    "    texts = []\n",
    "    if isinstance(contents, str):\n",
    "        texts.append(contents)\n",
    "    elif isinstance(contents, dict):\n",
    "        for k, v in contents.items():\n",
    "            texts.extend(extract_text_from_contents(v))\n",
    "    elif isinstance(contents, list):\n",
    "        for item in contents:\n",
    "            texts.extend(extract_text_from_contents(item))\n",
    "    return texts\n",
    "\n",
    "\n",
    "# simple weak-supervision keyword rules\n",
    "KEYWORDS = {\n",
    "    \"Strength\": [\"strong\", \"leading\", \"advantage\", \"growth\", \"robust\", \"increase in\", \"strength\"],\n",
    "    \"Weakness\": [\"decline\", \"risk\", \"cost\", \"vulnerable\", \"loss\", \"decrease\", \"weak\"],\n",
    "    \"Opportunity\": [\"opportunit\", \"potential\", \"emerging\", \"expand\", \"growth opportunity\", \"could benefit\"],\n",
    "    \"Threat\": [\"competition\", \"regulation\", \"lawsuit\", \"uncertain\", \"disruptor\", \"threat\", \"risk of\"]\n",
    "}\n",
    "\n",
    "\n",
    "def weak_label(sentence: str):\n",
    "    s = sentence.lower()\n",
    "    for label, kws in KEYWORDS.items():\n",
    "        for kw in kws:\n",
    "            if kw in s:\n",
    "                return label\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ba3e267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Portfolio in: sec_portfolio\n",
      "Loading submissions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading regular submissions:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading regular submissions: 100%|██████████| 2/2 [00:00<00:00, 106.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2 submissions\n",
      "Downloading filings (this can take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting query planning phase ---\n",
      "Analyzing request and splitting into manageable chunks...\n",
      "Fetching https://efts.sec.gov/LATEST/search-index?ciks=0000320193&forms=10-K%2C-10-K%2FA&startdt=2023-01-01&enddt=2024-12-31&from=0&size=1...\n",
      "Found 2 total documents to retrieve.\n",
      "Fetching https://efts.sec.gov/LATEST/search-index?ciks=0000320193&forms=10-K%2C-10-K%2FA&startdt=2023-01-01&enddt=2024-12-31&from=0&size=1...\n",
      "Planning: Analyzing query: cik=0000320193, forms=10-K,-10-K/A, dates=2023-01-01 to 2024-12-31 [2 hits]\n",
      "No additional forms to process with negation\n",
      "\n",
      "--- Starting query phase ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying documents [Rate: 0/s | 0 MB/s]:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://efts.sec.gov/LATEST/search-index?ciks=0000320193&forms=10-K%2C-10-K%2FA&startdt=2023-01-01&enddt=2024-12-31&from=0&size=100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying documents [Rate: 3.0/s | 0.01 MB/s]: 100%|██████████| 2/2 [00:00<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Query complete: 2 submissions retrieved ---\n",
      "\n",
      "--- Streaming complete: 2 EFTS results processed ---\n",
      "Loading submissions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading regular submissions: 100%|██████████| 2/2 [00:00<00:00, 102.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2 submissions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing submissions: 100%|██████████| 2/2 [00:00<00:00, 13025.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weak supervision keyword-based classification.\n",
      "Found 2 documents of type 10-K in portfolio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing: 0 submissions [00:01, ? submissions/s]<?, ?it/s]\n",
      "Streaming submissions: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1184 sentences from accession 000032019323000106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying sentences: 100%|██████████| 1184/1184 [00:00<00:00, 161966.54it/s]\n",
      "Processing filings:  50%|█████     | 1/2 [00:00<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1182 sentences from accession 000032019324000123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying sentences: 100%|██████████| 1182/1182 [00:00<00:00, 149179.05it/s]\n",
      "Processing filings: 100%|██████████| 2/2 [00:00<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done. Reports saved to sec_swot_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- MAIN PIPELINE -------------------------\n",
    "\n",
    "def analyze_portfolio(tickers=TICKERS, forms=FORMS, date_range=DATE_RANGE, portfolio_dir=PORTFOLIO_DIR, output_dir=OUTPUT_DIR):\n",
    "    ensure_dir(output_dir)\n",
    "    # create or reuse portfolio\n",
    "    print(\"Initializing Portfolio in:\", portfolio_dir)\n",
    "    port = Portfolio(portfolio_dir)\n",
    "\n",
    "    # download submissions for tickers\n",
    "    print(\"Downloading filings (this can take a while)...\")\n",
    "    try:\n",
    "        port.download_submissions(filing_date=date_range, submission_type=forms, ticker=tickers)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: download_submissions raised:\", e)\n",
    "        # continue; maybe files already present\n",
    "\n",
    "    # process local submissions (uses datamule's internal caching)\n",
    "    try:\n",
    "        port.process_submissions(lambda s: None)\n",
    "    except Exception:\n",
    "        # process_submissions may require callback; ignore if fails\n",
    "        pass\n",
    "    print(\"Using weak supervision keyword-based classification.\")\n",
    "\n",
    "\n",
    "    # iterate documents of requested type\n",
    "    docs = list(port.document_type(forms[0]))\n",
    "    print(f\"Found {len(docs)} documents of type {forms[0]} in portfolio.\")\n",
    "\n",
    "    summary_index = []\n",
    "\n",
    "    for doc in tqdm(docs, desc=\"Processing filings\"):\n",
    "        try:\n",
    "            doc.parse()\n",
    "        except Exception:\n",
    "            # parse may fail for some docs - skip\n",
    "            print(\"Warning: parse failed for a document; skipping\")\n",
    "            continue\n",
    "\n",
    "                # Fix metadata extraction\n",
    "        meta = doc.data.get('metadata', {})\n",
    "        doc_content = doc.data.get('document', {})\n",
    "\n",
    "        # Improved metadata extraction\n",
    "        accession = (meta.get('accession_number') or \n",
    "                    meta.get('accession') or \n",
    "                    doc.__dict__.get('accession') or \n",
    "                    getattr(doc, 'accession_number', None) or\n",
    "                    'unknown')\n",
    "        \n",
    "        # For Apple filings, manually set the ticker if we can identify it\n",
    "        cik = (meta.get('cik') or \n",
    "               getattr(doc, 'cik', None) or\n",
    "               doc.__dict__.get('cik'))\n",
    "        \n",
    "        # Map known CIKs to tickers\n",
    "        cik_to_ticker = {\n",
    "            '0000320193': 'AAPL',\n",
    "            '320193': 'AAPL'\n",
    "        }\n",
    "        \n",
    "        ticker = (meta.get('ticker') or \n",
    "                 meta.get('symbol') or \n",
    "                 cik_to_ticker.get(str(cik)) if cik else None or\n",
    "                 TICKERS[0] if len(TICKERS) == 1 else 'UNKNOWN')  # Use config ticker if only one\n",
    "        \n",
    "        filing_date = (meta.get('filing_date') or \n",
    "                      meta.get('filed_as_of_date') or\n",
    "                      getattr(doc, 'filing_date', None))\n",
    "        \n",
    "        # Debug: Print what we found\n",
    "        # collect sentences\n",
    "        sentences = []\n",
    "        for part_id, part in doc_content.items():\n",
    "            # part is a dict with 'contents'\n",
    "            if isinstance(part, dict):\n",
    "                contents = part.get('contents', {})\n",
    "                texts = extract_text_from_contents(contents)\n",
    "                for t in texts:\n",
    "                    tclean = clean_text(t)\n",
    "                    if len(tclean) >= MIN_SENTENCE_LENGTH:\n",
    "                        sents = split_sentences(tclean)\n",
    "                        sentences.extend(sents)\n",
    "\n",
    "        # fallback: if no sentences found, try reading any string values directly from doc_content\n",
    "        if not sentences:\n",
    "            for part_id, part in doc_content.items():\n",
    "                if isinstance(part, str) and len(part) > 30:\n",
    "                    sents = split_sentences(part)\n",
    "                    sentences.extend(sents)\n",
    "\n",
    "        print(f\"Extracted {len(sentences)} sentences from accession {accession}\")\n",
    "\n",
    "        if not sentences:\n",
    "            print(\"No textual sentences found for this filing - skipping output generation.\")\n",
    "            continue\n",
    "\n",
    "        # classify sentences\n",
    "        records = []\n",
    "        for sent in tqdm(sentences, desc=\"Classifying sentences\"):\n",
    "            lab = weak_label(sent)\n",
    "            score = 1.0 if lab else 0.0\n",
    "            if lab:\n",
    "                records.append({\"sentence\": sent, \"label\": lab, \"score\": score})\n",
    "\n",
    "        # prepare DataFrame and per-label grouping\n",
    "        df = pd.DataFrame(records)\n",
    "        if df.empty:\n",
    "            print(\"No records after labeling - skipping.\")\n",
    "            continue\n",
    "\n",
    "        # save per-filing CSV\n",
    "                # save per-filing CSV\n",
    "        out_csv = Path(output_dir) / f\"swot_{ticker}_{accession}.csv\"\n",
    "        df.to_csv(out_csv, index=False)\n",
    "        \n",
    "        # create a compact JSON report: top N bullets per label\n",
    "        def extract_key_phrases(sentences, max_phrases=3):\n",
    "            \"\"\"Extract key phrases from sentences using simple frequency analysis\"\"\"\n",
    "            from collections import Counter\n",
    "            import re\n",
    "    \n",
    "            # Combine all sentences and extract meaningful words\n",
    "            text = \" \".join(sentences).lower()\n",
    "            words = re.findall(r'\\b[a-z]{4,}\\b', text)  # Words with 4+ characters\n",
    "    \n",
    "            # Filter out common words\n",
    "            stopwords = {'that', 'with', 'have', 'this', 'will', 'from', 'they', 'been', 'said', 'each', 'which', 'their', 'there', 'these', 'those', 'would', 'could', 'should', 'other', 'such', 'more', 'also', 'may', 'can', 'its', 'our', 'us', 'we', 'you', 'your', 'the', 'and', 'or', 'but', 'so', 'if', 'when', 'where', 'how', 'what', 'who', 'why'}\n",
    "            meaningful_words = [w for w in words if w not in stopwords]\n",
    "    \n",
    "            # Get most common words as key phrases\n",
    "            counter = Counter(meaningful_words)\n",
    "            return [word for word, count in counter.most_common(max_phrases)]\n",
    "\n",
    "        report = {}\n",
    "        total_classified = len(df)\n",
    "        \n",
    "        for lab in candidate_labels:\n",
    "            lab_df = df[df['label'] == lab].sort_values('score', ascending=False)\n",
    "            all_sentences = lab_df['sentence'].tolist()\n",
    "            bullets = all_sentences[:3]  # Only top 3 sentences\n",
    "            key_phrases = extract_key_phrases(all_sentences[:10])  # Extract from top 10\n",
    "            \n",
    "            report[lab] = {\n",
    "                \"count\": len(lab_df), \n",
    "                \"top_bullets\": bullets,\n",
    "                \"key_themes\": key_phrases,\n",
    "                \"summary\": f\"{len(lab_df)} {lab.lower()} indicators found\"\n",
    "            }\n",
    "\n",
    "        report_meta = {\"ticker\": ticker, \"cik\": cik, \"accession\": accession, \"filing_date\": filing_date}\n",
    "        out_json = Path(output_dir) / f\"swot_report_{ticker}_{accession}.json\"\n",
    "        with open(out_json, 'w', encoding='utf-8') as fh:\n",
    "            json.dump({\"meta\": report_meta, \"report\": report}, fh, indent=2, ensure_ascii=False)\n",
    "\n",
    "        summary_index.append({**report_meta, \"csv\": str(out_csv), \"json\": str(out_json)})\n",
    "\n",
    "    # master index\n",
    "    with open(Path(output_dir)/\"index.json\", 'w', encoding='utf-8') as fh:\n",
    "        json.dump(summary_index, fh, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"All done. Reports saved to\", output_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ensure_dir(OUTPUT_DIR)\n",
    "    analyze_portfolio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
